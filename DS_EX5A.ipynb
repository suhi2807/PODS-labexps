{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut5Ag2Zvoo6a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTzC-f0lpDLk",
        "outputId": "8bde0a8a-b470-4202-fc5b-6c79f6d5a477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Reviews.csv\", on_bad_lines='skip', engine='python')\n",
        "data=pd.DataFrame(df)"
      ],
      "metadata": {
        "id": "qTx8tjnbpFnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = df['Text']"
      ],
      "metadata": {
        "id": "7b0ENNGipHOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = reviews.dropna()"
      ],
      "metadata": {
        "id": "dDcn-E5KpbRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = reviews[:10000]"
      ],
      "metadata": {
        "id": "Z-b3_vG0pcyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    #\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "reviews = reviews.apply(preprocess)"
      ],
      "metadata": {
        "id": "l_8UDSgjpegh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "kxwizIV1phMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "kzKvhbzApoRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "reviews_tokens = reviews.apply(tokenize)"
      ],
      "metadata": {
        "id": "Hr-vdwRWpqhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_and_ner(tokens):\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    ner_tree = ne_chunk(pos_tags)\n",
        "    return pos_tags, ner_tree\n"
      ],
      "metadata": {
        "id": "_xxkwStppsUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "example_tokens = reviews_tokens.iloc[0]\n",
        "pos_tags, ner_tree = pos_and_ner(example_tokens)\n",
        "\n",
        "print(\"Tokens:\", example_tokens)\n",
        "print(\"\\nPOS Tags:\", pos_tags)\n",
        "print(\"\\nNamed Entity Recognition Tree:\")\n",
        "print(ner_tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBx6JN6ypzCt",
        "outputId": "23ab6942-28ff-4665-c718-751269e4afb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['bought', 'several', 'vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'product', 'looks', 'like', 'stew', 'processed', 'meat', 'smells', 'better', 'labrador', 'finicky', 'appreciates', 'product', 'better']\n",
            "\n",
            "POS Tags: [('bought', 'VBD'), ('several', 'JJ'), ('vitality', 'NN'), ('canned', 'VBD'), ('dog', 'JJ'), ('food', 'NN'), ('products', 'NNS'), ('found', 'VBD'), ('good', 'JJ'), ('quality', 'NN'), ('product', 'NN'), ('looks', 'VBZ'), ('like', 'IN'), ('stew', 'NN'), ('processed', 'VBN'), ('meat', 'NN'), ('smells', 'NNS'), ('better', 'RBR'), ('labrador', 'NN'), ('finicky', 'JJ'), ('appreciates', 'VBZ'), ('product', 'NN'), ('better', 'RBR')]\n",
            "\n",
            "Named Entity Recognition Tree:\n",
            "(S\n",
            "  bought/VBD\n",
            "  several/JJ\n",
            "  vitality/NN\n",
            "  canned/VBD\n",
            "  dog/JJ\n",
            "  food/NN\n",
            "  products/NNS\n",
            "  found/VBD\n",
            "  good/JJ\n",
            "  quality/NN\n",
            "  product/NN\n",
            "  looks/VBZ\n",
            "  like/IN\n",
            "  stew/NN\n",
            "  processed/VBN\n",
            "  meat/NN\n",
            "  smells/NNS\n",
            "  better/RBR\n",
            "  labrador/NN\n",
            "  finicky/JJ\n",
            "  appreciates/VBZ\n",
            "  product/NN\n",
            "  better/RBR)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v_fPvuoNp184"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}